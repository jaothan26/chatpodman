# Production-grade deployment with GPU support
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-server
  labels:
    app: ai-chat
spec:
  replicas: 2
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: llama
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: ["llama"]
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: llama
        image: localhost/llama-model-service
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
        resources:
          limits:
            cpu: 4
            memory: 16Gi
            nvidia.com/gpu: 1  # GPU acceleration
        volumeMounts:
        - mountPath: /models
          name: model-storage
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc
---
# LoadBalancer service with internal-only access
apiVersion: v1
kind: Service
metadata:
  name: llama-service
spec:
  type: LoadBalancer
  loadBalancerIP: "10.0.0.100"  # Static IP for internal LB
  ports:
  - port: 8000
    targetPort: 8000
  selector:
    app: llama
